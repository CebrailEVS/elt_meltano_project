version: 1
default_environment: dev
project_id: 70d9670f-9691-400e-aaf2-49a378faa6d2

environments:
- name: dev
- name: staging
- name: prod

plugins:
  extractors:
  - name: tap-rest-api-msdk
    variant: widen
    pip_url: tap-rest-api-msdk
    config:
      api_url: https://api.yuman.io
      auth_method: bearer_token
      bearer_token: ${YUMAN_BEARER_TOKEN}
      pagination_request_style: page_number_paginator
      pagination_response_style: default
      # Réduire la taille des pages pour moins de mémoire
      pagination_page_size: 50
      pagination_initial_offset: 1
      pagination_limit_per_page_param: per_page
      pagination_page_number_field: page
      next_page_token_path: $.page
      request_timeout: 300
      request_records_limit: 1000
      streams:
      - name: yuman_clients
        path: /v1/clients
        records_path: $.items[*]
        primary_keys: [id]
        schema: extract/schemas/yuman_clients.json
        replication_key: updated_at
        params:
          per_page: 50
          embed: fields

      - name: yuman_sites
        path: /v1/sites
        records_path: $.items[*]
        primary_keys: [id]
        replication_key: updated_at
        params:
          per_page: 50
          embed: fields


  - name: tap-oracle
    variant: s7clarke10
    pip_url: git+https://github.com/s7clarke10/pipelinewise-tap-oracle.git
    config:
      host: evs.easyweb.distrilog.net
      port: 1521
      service_name: DLOG
      user: EVS_RO
      password: ${ORACLE_PASSWORD}
      filter_schemas: EVS
      filter_tables: [EVS-CONTACT, EVS-COMPANY, EVS-TASK_HAS_PRODUCT]
      ora_python_driver_type: thin
      default_replication_method: INCREMENTAL
      # Optimisations Oracle pour batch processing
      batch_config:
        encoding:
          format: jsonl
          compression: gzip
        batch_size: 10000  # Traiter par batch de 10k enregistrements
      # Configuration de streaming pour éviter de charger tout en mémoire
      stream_buffer_size: 1000
      # Timeout et connexion
      query_timeout: 1800  # 30 minutes timeout
      connection_pool_size: 5
    select:
    - EVS-CONTACT.IDCONTACT
    - EVS-CONTACT.IDCOMPANY
    - EVS-CONTACT.MODIFICATION_DATE
    - EVS-COMPANY.*
    - EVS-TASK_HAS_PRODUCT.*
    metadata:
      EVS-CONTACT:
        replication-method: INCREMENTAL
        replication-key: MODIFICATION_DATE
      EVS-COMPANY:
        replication-method: INCREMENTAL
        replication-key: MODIFICATION_DATE
      EVS-TASK_HAS_PRODUCT:
        replication-method: INCREMENTAL
        replication-key: MODIFICATION_DATE

  loaders:
  - name: target-postgres
    variant: meltanolabs
    pip_url: meltanolabs-target-postgres
    config:
      host: ${POSTGRES_HOST}
      port: ${POSTGRES_PORT}
      user: ${POSTGRES_USER}
      password: ${POSTGRES_PASSWORD}
      database: ${POSTGRES_DATABASE}
      default_target_schema: ${POSTGRES_SCHEMA}
      validate_records: false
      # Optimisations PostgreSQL
      batch_config:
        encoding:
          format: jsonl
          compression: gzip
        batch_size: 10000  # Même taille que l'extractor
      # Configuration pour les gros volumes
      max_batch_age: 300
      batch_size_rows: 5000
      flush_all_streams: true
      # Optimisations de performance PostgreSQL
      add_record_metadata: true
      hard_delete: false
  transformers:
  - name: dbt
    variant: dbt-labs
    pip_url: dbt-core~=1.3.0 dbt-postgres~=1.3.0 dbt-redshift~=1.3.0 
      dbt-snowflake~=1.3.0 dbt-bigquery~=1.3.0
